---
title: "Databricks ã§ uv ã‚’æ´»ç”¨ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’ç®¡ç†ã™ã‚‹"
emoji: "ğŸ´"
type: "tech"
topics:
  - "databricks"
  - "python"
  - "uv"
publication_name: "genda_jp"
published: true
published_at: 2025-12-11 07:00
---

## 1. ã¯ã˜ã‚ã«

æ ªå¼ä¼šç¤¾GENDA ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ / MLOps ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã® uma-chan ã§ã™ã€‚
ã“ã®è¨˜äº‹ã¯ GENDA Advent Calendar 2025 ã‚·ãƒªãƒ¼ã‚º4 Day 11 ã®è¨˜äº‹ã§ã™ã€‚

@[card](https://qiita.com/advent-calendar/2025/genda)

æœ¬è¨˜äº‹ã§ã¯ã€Databricks ã§ uv ã‚’ä½¿ã£ã¦ä¾å­˜é–¢ä¿‚ã‚’ç®¡ç†ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## 2. uv ã¨ã¯

uv ã¯ Rust è£½ã®é«˜é€Ÿãª Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ã§ã™ã€‚

@[card](https://docs.astral.sh/uv/)

ä¸»ãªç‰¹å¾´

- pip ã®10-100å€é«˜é€Ÿ
- `uv.lock` ã«ã‚ˆã‚‹å†ç¾å¯èƒ½ãªä¾å­˜é–¢ä¿‚ç®¡ç†
- Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚‚å¯èƒ½

## 3. uv ã‚’ä½¿ã£ãŸ Databricks ã§ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†

### 3.1. requirements.txt äº‹å‰ç”Ÿæˆ

`uv.lock` ã‹ã‚‰ `requirements.txt` ã‚’ç”Ÿæˆã—ã€ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã—ã¦ãŠãã¾ã™ã€‚

```bash
uv export --no-hashes --no-dev > requirements.txt
```

Databricks ã§ã¯äº‹å‰ç”Ÿæˆã—ãŸ requirements.txt ã‚’ä½¿ã„ã¾ã™ã€‚

```python
%pip install -r requirements.txt
```

pre-commit ã§è‡ªå‹•åŒ–ã™ã‚‹ã¨ã€ç”Ÿæˆå¿˜ã‚Œã‚„æ‰‹å‹•ç·¨é›†ã«ã‚ˆã‚‹æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã’ã¾ã™ã€‚

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: uv-export
        name: uv export
        entry: uv export --no-hashes --no-dev -o requirements.txt
        language: system
        files: ^(pyproject\.toml|uv\.lock)$
        pass_filenames: false
```

ãƒ¡ãƒªãƒƒãƒˆ

- PR ã§ä¾å­˜é–¢ä¿‚ã®å¤‰æ›´ãŒè¦‹ã‚„ã™ã„

### 3.2. requirements.txt å‹•çš„ç”Ÿæˆ

uv ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠãå‹•çš„ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

```python
%pip install uv
```

```python
import subprocess
result = subprocess.run(
    ["uv", "export", "--no-hashes", "--no-dev"],
    cwd="/Workspace/Repos/<user>/<repo>",
    capture_output=True,
    text=True
)
print(result.stdout)
```

ãƒ¡ãƒªãƒƒãƒˆ

- requirements.txt ã®ç®¡ç†ãŒä¸è¦
- å¸¸ã« uv.lock ã‹ã‚‰ç”Ÿæˆã™ã‚‹ãŸã‚æ•´åˆæ€§ãŒä¿è¨¼ã•ã‚Œã‚‹

ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

- æ¯å› uv ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒç™ºç”Ÿã™ã‚‹

### 3.3. uv sync / uv run ã«ã¤ã„ã¦

ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã§ã¯ `uv sync` ã§ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã—ã€
`uv run` ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚
Databricks ã§ã‚‚ `--active` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ã†ã“ã¨ã§åŒæ§˜ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒä½¿ãˆã¾ã™ã€‚

```python
%pip install uv
```

```python
import subprocess

# uv sync --active ã§ Databricks æ—¢å­˜ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
result = subprocess.run(
    ["uv", "sync", "--no-dev", "--active"],
    capture_output=True,
    text=True
)
```

```python
# uv run --active ã§å®Ÿè¡Œ
result = subprocess.run(
    ["uv", "run", "--no-dev", "--active", "python", "-c",
     "import httpx; print(httpx.__version__)"],
    capture_output=True,
    text=True
)
print(result.stdout)  # 0.28.1
```

`--active` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€æ–°ã—ã„ `.venv` ã‚’ä½œæˆã›ãšã€
ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªä»®æƒ³ç’°å¢ƒï¼ˆ`VIRTUAL_ENV` ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚ŒãŸç’°å¢ƒï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

Databricks ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒã§ã¯ã€æ—¢ã« `/local_disk0/` ä¸Šã«
ä»®æƒ³ç’°å¢ƒãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ãªã£ã¦ã„ã‚‹ãŸã‚ã€
`/Workspace`ï¼ˆNFSï¼‰ä¸Šã« `.venv` ã‚’ä½œæˆã™ã‚‹éš›ã®å•é¡Œã‚’å›é¿ã§ãã¾ã™ã€‚

#### 3.3.1. --active ä½¿ç”¨æ™‚ã®æ³¨æ„ç‚¹

- Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ•´åˆæ€§
    - `requires-python` ã‚’ DBR ã® Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆã‚ã›ã‚‹
    - ä¾‹: DBR 17.3 LTS ã¯ Python 3.12.3
- DBR ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã®ç«¶åˆ
    - `dependencies` ã«ã¯ DBR ã«ãªã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã¿è¨˜è¼‰ã™ã‚‹
    - pandas, numpy ç­‰ã¯ DBR ã«ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†èµ·å‹•ã§ãƒªã‚»ãƒƒãƒˆ
    - ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†èµ·å‹•ã§æ¶ˆãˆã‚‹
    - ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œæ™‚ã«æ¯å› `uv sync --active` ãŒå¿…è¦

#### 3.3.2. --active ãªã—ã®å ´åˆï¼ˆå‚è€ƒï¼‰

~~`--active` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãªã—ã§ `uv sync` ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€
`/Workspace` ä¸Šã« `.venv` ãŒä½œæˆã•ã‚Œã¾ã™ã€‚~~

~~`uv sync` ã¯æˆåŠŸã—ã¾ã™ãŒã€`uv run` ã¯å¤±æ•—ã—ã¾ã™ã€‚
ã“ã‚Œã¯ `/Workspace` ãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚Šã€
`.venv/bin/python` ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã®æ­£è¦åŒ–ã§
`os error 18` (Invalid cross-device link) ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã§ã™ã€‚~~

~~`--link-mode=copy` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ã‚‚ã“ã®å•é¡Œã¯è§£æ±ºã§ãã¾ã›ã‚“ã€‚
`--link-mode=copy` ã¯ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã®ãƒªãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ¶å¾¡ã—ã¾ã™ãŒã€
Python ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã¯åˆ¥ã®ä»•çµ„ã¿ã§ä½œã‚‰ã‚Œã‚‹ãŸã‚ã§ã™ã€‚~~

## 4. pyproject.toml ã®æ§‹æˆ

```toml
[project]
name = "databricks-project"
version = "0.1.0"
requires-python = ">=3.12"
dependencies = [
    # Databricks ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
]

[dependency-groups]
dev = [
    "pytest",
    "ruff",
]
dbr = [
    # DBR 17.3 LTS ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
    # https://docs.databricks.com/aws/en/release-notes/runtime/17.3lts
    "pandas==2.2.3",
    "numpy==2.1.3",
    "pyspark==4.0.0",
]
```

ãƒã‚¤ãƒ³ãƒˆ

- `dependencies`: Databricks ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
- `dev`: ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ„ãƒ¼ãƒ«
- `dbr`: Databricks Runtime (DBR) ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
    - ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆæ€§ã‚’ä¿ã¤ãŸã‚ã«è¨˜è¼‰
    - `uv export --no-dev` ã§ã‚‚å«ã¾ã‚Œãªã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é™¤å¤–ï¼‰

## 5. ã¾ã¨ã‚

uv ã‚’ä½¿ã†ã“ã¨ã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã§ãã¾ã™ã€‚

ã¾ã  uv ã‚’ä½¿ã£ãŸã“ã¨ãŒãªã„å ´åˆã¯ãœã²è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

## 6. é–¢é€£è¨˜äº‹

@[card](https://zenn.dev/genda_jp/articles/2025-12-10-organize-databricks-notebook-management)

@[card](https://zenn.dev/genda_jp/articles/2025-12-06-ai-guardrails-local-cloud)

## 7. å‚è€ƒ

- <https://docs.astral.sh/uv/>
- <https://docs.astral.sh/uv/reference/cli/#uv-export>
