---
title: "å…¨éƒ¨ Databricks ã«é›†ç´„ã—ã¦èª°ã§ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰æ“ä½œã§ãã‚‹ç’°å¢ƒ â€• ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»SQLãƒ»Notebook"
emoji: "ğŸ¦†"
type: "tech"
topics:
  - "databricks"
  - "devcontainer"
  - "goose"
  - "jupyter"
  - "mcp"
publication_name: "genda_jp"
published: true
published_at: 2026-01-27 07:30
---

## 1. ã¯ã˜ã‚ã«

æ ªå¼ä¼šç¤¾GENDA ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ / MLOps ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã® uma-chan ã§ã™ã€‚

AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Notebookã€SQL ã‚’ã™ã¹ã¦ Databricks ã«é›†ç´„ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰æ“ä½œã§ãã‚‹ Dev Container ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ OSS ã¨ã—ã¦å…¬é–‹ã—ã¾ã—ãŸã€‚

@[card](https://github.com/i9wa4/databricks-goose-starter)

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ¡ãƒªãƒƒãƒˆãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

- æŠ€è¡“éƒ¨é–€ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã§ã‚‚è‡ªç„¶è¨€èªã§ AI/Notebook/SQL ã‚’æ“ä½œã§ãã‚‹
- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®æ§‹ç¯‰ãŒä¸è¦ (Dev Container + Databricks ã§ãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œ)
- å€‹äººèªè¨¼ã§è‡ªåˆ†ã®æ¨©é™ç¯„å›²å†…ã®ã¿æ“ä½œå¯èƒ½ (ç®¡ç†è€…ã®è¨­å®šä¸è¦)

æœ¬è¨˜äº‹ã§ã¯ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ©Ÿèƒ½ã¨ä½¿ã„æ–¹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

ãªãŠæœ¬è¨˜äº‹ã¯ä¸‹è¨˜ã‚¤ãƒ™ãƒ³ãƒˆã§ã®ç™»å£‡å†…å®¹ã‚’è£œå®Œã™ã‚‹ã‚‚ã®ã§ã™ã€‚

@[card](https://jedai.connpass.com/event/379174/)

## 2. è§£æ±ºã—ãŸã„èª²é¡Œ

AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’çµ„ç¹”å†…ã«å±•é–‹ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªèª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚

- ç’°å¢ƒæ§‹ç¯‰ã®ãƒãƒ¼ãƒ‰ãƒ«ãŒé«˜ã„
- èªè¨¼è¨­å®šãŒè¤‡é›‘ (API ã‚­ãƒ¼ç®¡ç†ã€Service Principal ä½œæˆãªã©)
- æŠ€è¡“éƒ¨é–€ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ã¯æ•·å±…ãŒé«˜ã„

é–‹ç™ºè€…ã§ãªãã¦ã‚‚ AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½¿ãˆã‚‹ç’°å¢ƒã‚’ç›®æŒ‡ã—ã¾ã—ãŸã€‚

## 3. ãªãœç°¡å˜ã‹

### 3.1. Dev Container ã«ã‚ˆã‚‹ç’°å¢ƒæ§‹ç¯‰

Dev Container ã«ã‚ˆã‚Šç’°å¢ƒæ§‹ç¯‰ãŒè‡ªå‹•åŒ–ã•ã‚Œã€Databricks ã‚„ AI ç‰¹æœ‰ã®åˆæœŸè¨­å®šã‚’ä¸€èˆ¬çš„ãªæ‰‹é †ã«å¯„ã›ã‚‰ã‚Œã¾ã™ã€‚

### 3.2. OAuth U2M èªè¨¼

ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ OAuth U2M (User-to-Machine) èªè¨¼ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚

å¾“æ¥ã®æ–¹æ³•ã¨ã®æ¯”è¼ƒ

| èªè¨¼æ–¹å¼              | è¨­å®šã®è¤‡é›‘ã• | ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†   | æ¨©é™ç®¡ç†     |
| ---                   | ---          | ---            | ---          |
| Personal Access Token | ä¸­           | æ‰‹å‹•æ›´æ–°ãŒå¿…è¦ | ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ |
| Service Principal     | é«˜           | è‡ªå‹•           | SP å˜ä½      |
| OAuth U2M             | ä½           | è‡ªå‹•æ›´æ–°       | ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ |

OAuth U2M ã®ãƒ¡ãƒªãƒƒãƒˆ

- Service Principal ã®ä½œæˆãŒä¸è¦
- å€‹äººã®èªè¨¼æƒ…å ±ã§å³åº§ã«åˆ©ç”¨é–‹å§‹
- é–‹ç™ºè€…ã”ã¨ã®æ¨©é™ã§å‹•ä½œ (æœ€å°æ¨©é™ã®åŸå‰‡)
- ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ã®æ‰‹é–“ãŒä¸è¦ (OAuth è‡ªå‹•æ›´æ–°)

Dev Container èµ·å‹•æ™‚ã« `databricks auth login` ãŒå®Ÿè¡Œã•ã‚Œã€ãƒ–ãƒ©ã‚¦ã‚¶ã§èªè¨¼ã™ã‚‹ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚

## 4. å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼

ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©ã—ã¦ã„ã¾ã™ã€‚

- ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã€ãƒ“ã‚¸ãƒã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã€å¤–éƒ¨å”åŠ›è€…
- ä»Šã™ã AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç’°å¢ƒã‚’ä½¿ã„ãŸã„äºº

é€†ã«é©ã—ã¦ã„ãªã„ã‚±ãƒ¼ã‚¹

- å¤§é‡ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¶ˆè²»ã™ã‚‹ãƒ˜ãƒ“ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®åˆ©ç”¨åˆ¶é™ãŒå¿…è¦ãªå ´åˆ

## 5. Databricks ã«é›†ç´„ã™ã‚‹ã‚‚ã®

### 5.1. AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Goose + Mosaic AI Gateway)

Goose ã¯ Block ç¤¾ãŒé–‹ç™ºã—ãŸ AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚Databricks Mosaic AI Gateway çµŒç”±ã§ Claude ã‚„ GPT ãªã©ã® LLM ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

@[card](https://github.com/block/goose)

Goose ã‚’é¸å®šã—ãŸç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- Databricks ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ (OAuth U2M ã§ãã®ã¾ã¾ä½¿ãˆã‚‹)
- OSS ã§ã‚ã‚ŠãªãŒã‚‰ä¼æ¥­ (Block ç¤¾) ãŒä¸»ä½“é–‹ç™º
- æ´»ç™ºãªé–‹ç™º

Goose ã¯ä»¥ä¸‹ã®ã“ã¨ãŒã§ãã¾ã™ã€‚

- ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ (`uv run jupyter execute` ã‚’å«ã‚€)
- Notebook ã®èª­ã¿æ›¸ã
- Codebase ã¨ã®å¯¾è©±

æŠ€è¡“éƒ¨é–€ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã§ã‚‚ã€è‡ªç„¶è¨€èªã§ã€Œã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®é›†è¨ˆã‚’ã—ã¦ã€ã€ŒNotebook ã‚’å®Ÿè¡Œã—ã¦ã€ã¨ä¾é ¼ã™ã‚‹ã ã‘ã§æ“ä½œã§ãã¾ã™ã€‚

### 5.2. Notebook (jupyter-databricks-kernel)

jupyter-databricks-kernel ã«ã‚ˆã‚Šã€Notebook ã®ã‚³ãƒ¼ãƒ‰ã‚’ Databricks ã‚¯ãƒ©ã‚¹ã‚¿ä¸Šã§å®Œå…¨ãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œã§ãã¾ã™ã€‚

```bash
uv run jupyter execute notebooks/sample.ipynb --inplace --kernel_name=databricks
```

@[card](https://github.com/i9wa4/jupyter-databricks-kernel)

Goose ã«ã¯ jupyter-notebook ã‚¹ã‚­ãƒ«ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ãŠã‚Šã€ã€Œã“ã® Notebook ã‚’å®Ÿè¡Œã—ã¦ã€ã¨ä¾é ¼ã™ã‚‹ã ã‘ã§ä¸Šè¨˜ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’å–å¾—ã—ã¦ãã‚Œã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ãƒªã‚½ãƒ¼ã‚¹ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

### 5.3. SQL (mcp-databricks-server)

mcp-databricks-server ãŒäº‹å‰è¨­å®šã•ã‚Œã¦ãŠã‚Šã€Goose ã‹ã‚‰ Databricks SQL Warehouse ã«å¯¾è©±çš„ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

- SQL ã‚¯ã‚¨ãƒªå®Ÿè¡Œ (Databricks SQL Warehouse çµŒç”±)
- ã‚«ã‚¿ãƒ­ã‚°ã€ã‚¹ã‚­ãƒ¼ãƒã€ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ (Unity Catalog)
- ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®ç¢ºèª
- ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªãƒãƒ¼ã‚¸æƒ…å ±ã®å–å¾—

å®‰å…¨ã®ãŸã‚ã€DROP/DELETE/TRUNCATE ãªã©ã®ç ´å£Šçš„ãª SQL ã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚

@[card](https://github.com/i9wa4/mcp-databricks-server)

SQL ã‚’æ›¸ã‘ãªãã¦ã‚‚ã€Œå£²ä¸Šã®æœˆåˆ¥æ¨ç§»ã‚’è¦‹ãŸã„ã€ã¨ä¼ãˆã‚‹ã ã‘ã§ã€Goose ãŒé©åˆ‡ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚

## 6. ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ã®æ“ä½œæ–¹æ³•

### 6.1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

å‰ææ¡ä»¶

- VS Code + [Dev Containers æ‹¡å¼µæ©Ÿèƒ½](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- Docker Desktop ã¾ãŸã¯ Docker ãƒ‡ãƒ¼ãƒ¢ãƒ³
- Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ (Mosaic AI Gateway æœ‰åŠ¹)

æ‰‹é †

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
1. `.env.example` ã‚’ `.env` ã«ã‚³ãƒ”ãƒ¼ã—ã¦ç·¨é›†

   ```bash
   cp .env.example .env
   ```

   ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š

   | å¤‰æ•°                          | èª¬æ˜                          |
   | ---                           | ---                           |
   | `DATABRICKS_HOST`             | Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ URL |
   | `DATABRICKS_CLUSTER_ID`       | Notebook å®Ÿè¡Œç”¨ã‚¯ãƒ©ã‚¹ã‚¿ ID    |
   | `DATABRICKS_SQL_WAREHOUSE_ID` | SQL å®Ÿè¡Œç”¨ Warehouse ID       |
   | `GOOSE_PROVIDER`              | LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ (databricks)   |
   | `GOOSE_MODEL`                 | ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«                |

1. VS Code ã§ãƒªãƒã‚¸ãƒˆãƒªã‚’é–‹ã
1. ã€ŒReopen in Containerã€ã‚’ã‚¯ãƒªãƒƒã‚¯
1. è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å¾…ã¤
   - mise ã¨ãƒ„ãƒ¼ãƒ« (goose, databricks cli, uv) ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
   - Python ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
   - Databricks OAuth èªè¨¼ (ãƒ–ãƒ©ã‚¦ã‚¶ãŒé–‹ãã¾ã™)
   - Goose ã® MCP è¨­å®š

### 6.2. ä½¿ã„æ–¹

```bash
goose
```

ã“ã‚Œã ã‘ã§ AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å¯¾è©±ã§ãã¾ã™ã€‚

ä¾‹

- ã€Œsamples.nyctaxi.trips ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’æ•™ãˆã¦ã€
- ã€Œnotebooks/sample.ipynb ã‚’å®Ÿè¡Œã—ã¦ã€

æŠ€è¡“çš„ãªçŸ¥è­˜ãŒãªãã¦ã‚‚ã€ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è‡ªç„¶è¨€èªã§ä¼ãˆã‚‹ã ã‘ã§æ“ä½œã§ãã¾ã™ã€‚

## 7. é‹ç”¨

### 7.1. äºˆç®—ç®¡ç†

ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã®ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ã‚’ç›£è¦–ã—ã€äºˆç®—è¶…éæ™‚ã«è‡ªå‹•ã§ã‚¢ã‚¯ã‚»ã‚¹ã‚’åˆ¶é™ã™ã‚‹ä»•çµ„ã¿ã‚’ Databricks Job ã§å®Ÿè£…ã§ãã¾ã™ã€‚

ãªãŠã€äºˆç®—ç®¡ç† Job ã¯ Databricks Job ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã€Spark ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªå‹•çš„ã«èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚Dev Container ã® OAuth U2M èªè¨¼ã¨ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã—ã¾ã™ã€‚

ä»•çµ„ã¿

- `system.serving.endpoint_usage` ãƒ†ãƒ¼ãƒ–ãƒ«ã§ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ã‚’é›†è¨ˆ
- Mosaic AI Gateway ã® `rate_limits` API ã§ `calls=0` ã‚’è¨­å®šã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒ–ãƒ­ãƒƒã‚¯
- æœˆåˆã« `rate_limits` ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è§£é™¤

:::details budget-monitor Job ã®ã‚³ãƒ¼ãƒ‰ä¾‹

äºˆç®—è¶…éãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œå‡ºã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ Job ã§ã™ã€‚

```python:budget_monitor.py
"""
Budget Monitor Job - Detects budget overages and blocks users via rate limits.
"""

import os
import sys
from datetime import datetime

import requests

# Configuration
BUDGET_TOKENS = 10_000_000  # 10 million tokens
ENDPOINT_NAME = "databricks-claude-sonnet-4"
WAREHOUSE_ID = "your-sql-warehouse-id"  # SQL Warehouse ã® ID ã«ç½®ãæ›ãˆ


def log(message: str):
    """Print timestamped log message."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def get_databricks_credentials():
    """Get Databricks host and token."""
    try:
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.getOrCreate()
        host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")
        from pyspark.dbutils import DBUtils

        dbutils = DBUtils(spark)
        token = (
            dbutils.notebook.entry_point.getDbutils()
            .notebook()
            .getContext()
            .apiToken()
            .get()
        )
        return host, token
    except Exception as e:
        log(f"Warning: Could not get credentials from Spark context: {e}")

    host = os.environ.get("DATABRICKS_HOST")
    token = os.environ.get("DATABRICKS_TOKEN")
    if not host or not token:
        raise ValueError("DATABRICKS_HOST and DATABRICKS_TOKEN must be set")
    return host.rstrip("/"), token


def execute_sql(host: str, token: str, statement: str) -> list:
    """Execute SQL via Statement Execution API."""
    url = f"{host}/api/2.0/sql/statements"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload = {
        "warehouse_id": WAREHOUSE_ID,
        "statement": statement,
        "wait_timeout": "50s",
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    result = response.json()

    if result["status"]["state"] != "SUCCEEDED":
        raise RuntimeError(f"SQL execution failed: {result}")

    return result.get("result", {}).get("data_array", [])


def get_exceeded_users_with_usage(host: str, token: str) -> list:
    """Get users who exceeded budget with their token usage."""
    query = f"""
    SELECT
        requester,
        SUM(input_token_count) as input_tokens,
        SUM(output_token_count) as output_tokens,
        SUM(input_token_count + output_token_count) as total_tokens
    FROM system.serving.endpoint_usage
    WHERE request_time >= date_trunc('month', current_date())
    GROUP BY requester
    HAVING SUM(input_token_count + output_token_count) >= {BUDGET_TOKENS}
    ORDER BY total_tokens DESC
    """
    return execute_sql(host, token, query)


def block_users(host: str, token: str, users: list) -> bool:
    """Set rate_limits calls=0 for exceeded users. Returns success status."""
    if not users:
        log("No users to block")
        return True

    rate_limits = [
        {"calls": 0, "key": "user", "principal": user, "renewal_period": "minute"}
        for user in users
    ]
    config = {"rate_limits": rate_limits, "usage_tracking_config": {"enabled": True}}

    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}/ai-gateway"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    try:
        response = requests.put(url, headers=headers, json=config)
        response.raise_for_status()
        return True
    except Exception as e:
        log(f"ERROR: Failed to block users: {e}")
        return False


def main():
    print("=" * 60)
    log("Budget Monitor Job started")
    print("=" * 60)

    # Configuration
    print("\n[Configuration]")
    print(f"  Budget threshold: {BUDGET_TOKENS:,} tokens")
    print(f"  Target endpoint:  {ENDPOINT_NAME}")
    print(f"  SQL Warehouse:    {WAREHOUSE_ID}")

    host, token = get_databricks_credentials()
    print(f"  Workspace:        {host}")

    # Query exceeded users
    print("\n[Query Results]")
    log("Querying system.serving.endpoint_usage...")

    exceeded_users = get_exceeded_users_with_usage(host, token)

    if not exceeded_users:
        log("No users exceeding budget found")
    else:
        log(f"Found {len(exceeded_users)} user(s) exceeding budget:")
        print()
        print(f"  {'User':<40} {'Input':<15} {'Output':<15} {'Total':<15}")
        print(f"  {'-' * 40} {'-' * 15} {'-' * 15} {'-' * 15}")
        for row in exceeded_users:
            user, input_t, output_t, total_t = (
                row[0],
                int(row[1]),
                int(row[2]),
                int(row[3]),
            )
            print(f"  {user:<40} {input_t:>14,} {output_t:>14,} {total_t:>14,}")

    # Block users
    print("\n[Block Processing]")
    users_to_block = [row[0] for row in exceeded_users]

    if users_to_block:
        log(f"Blocking {len(users_to_block)} user(s)...")
        success = block_users(host, token, users_to_block)
        if success:
            log("SUCCESS: All users blocked via rate_limits (calls=0)")
        else:
            log("FAILED: Could not block users")
            sys.exit(1)
    else:
        log("No blocking action required")

    # Completion
    print()
    print("=" * 60)
    log("Budget Monitor Job completed")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

:::

:::details budget-monthly-reset Job ã®ã‚³ãƒ¼ãƒ‰ä¾‹

æœˆåˆã« rate_limits ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ Job ã§ã™ã€‚

```python:budget_monthly_reset.py
"""
Budget Monthly Reset Job - Resets rate limits at the beginning of each month.
"""

import os
import sys
from datetime import datetime

import requests

# Configuration
ENDPOINT_NAME = "databricks-claude-sonnet-4"


def log(message: str):
    """Print timestamped log message."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def get_databricks_credentials():
    """Get Databricks host and token."""
    try:
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.getOrCreate()
        host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")
        from pyspark.dbutils import DBUtils

        dbutils = DBUtils(spark)
        token = (
            dbutils.notebook.entry_point.getDbutils()
            .notebook()
            .getContext()
            .apiToken()
            .get()
        )
        return host, token
    except Exception as e:
        log(f"Warning: Could not get credentials from Spark context: {e}")

    host = os.environ.get("DATABRICKS_HOST")
    token = os.environ.get("DATABRICKS_TOKEN")
    if not host or not token:
        raise ValueError("DATABRICKS_HOST and DATABRICKS_TOKEN must be set")
    return host.rstrip("/"), token


def get_current_rate_limits(host: str, token: str) -> dict:
    """Get current AI Gateway configuration."""
    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}"
    headers = {"Authorization": f"Bearer {token}"}

    response = requests.get(url, headers=headers)
    response.raise_for_status()
    result = response.json()
    return result.get("ai_gateway", {})


def reset_rate_limits(host: str, token: str) -> bool:
    """Reset rate limits to unblock all users. Returns success status."""
    # WARNING: This PUT overwrites all rate_limits completely.
    # Check existing rate_limits before running if you have other limits.
    config = {"rate_limits": [], "usage_tracking_config": {"enabled": True}}

    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}/ai-gateway"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    try:
        response = requests.put(url, headers=headers, json=config)
        response.raise_for_status()
        return True
    except Exception as e:
        log(f"ERROR: Failed to reset rate limits: {e}")
        return False


def main():
    print("=" * 60)
    log("Budget Monthly Reset Job started")
    print("=" * 60)

    # Configuration
    print("\n[Configuration]")
    print(f"  Target endpoint: {ENDPOINT_NAME}")

    host, token = get_databricks_credentials()
    print(f"  Workspace:       {host}")

    # Get current state
    print("\n[Current State]")
    log("Fetching current rate_limits...")

    current_config = get_current_rate_limits(host, token)
    current_limits = current_config.get("rate_limits", [])

    if not current_limits:
        log("No rate_limits currently set")
    else:
        log(f"Found {len(current_limits)} rate_limit(s):")
        for limit in current_limits:
            principal = limit.get("principal", "N/A")
            calls = limit.get("calls", "N/A")
            key = limit.get("key", "N/A")
            print(f"  - {principal} (key={key}, calls={calls})")

    # Reset rate limits
    print("\n[Reset Processing]")
    log("Resetting rate_limits to empty...")

    success = reset_rate_limits(host, token)

    if success:
        log("SUCCESS: Rate limits cleared")
        if current_limits:
            log(f"Unblocked {len(current_limits)} user(s)")
        else:
            log("No users were blocked")
    else:
        log("FAILED: Could not reset rate limits")
        sys.exit(1)

    # Completion
    print()
    print("=" * 60)
    log("Budget Monthly Reset Job completed")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

:::

## 8. ã¾ã¨ã‚

AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Notebookã€SQL ã‚’ã™ã¹ã¦ Databricks ã«é›†ç´„ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ãŒå®Ÿç¾ã§ãã¾ã™ã€‚

- ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ AI/Notebook/SQL ã‚’çµ±ä¸€çš„ã«æ“ä½œ
- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®æ§‹ç¯‰ãŒä¸è¦
- OAuth U2M èªè¨¼ã ã‘ã§å³åº§ã«åˆ©ç”¨é–‹å§‹
- æŠ€è¡“éƒ¨é–€ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã§ã‚‚ AI ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¯èƒ½

ãœã²ãŠè©¦ã—ãã ã•ã„ã€‚Issue ã§ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ææ¡ˆãªã©ãŠæ°—è»½ã«ã©ã†ãã€‚

@[card](https://github.com/i9wa4/databricks-goose-starter)

### 8.1. é–¢é€£è¨˜äº‹

@[card](https://zenn.dev/genda_jp/articles/2025-12-13-jupyter-databricks-kernel-oss-dev)

@[card](https://zenn.dev/genda_jp/articles/2025-12-19-databricks-notebook-ai-ready)

@[card](https://zenn.dev/genda_jp/articles/2025-12-24-mcp-databricks-server-v2)

### 8.2. é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

@[card](https://github.com/block/goose)

@[card](https://github.com/i9wa4/mcp-databricks-server)

@[card](https://github.com/i9wa4/jupyter-databricks-kernel)
