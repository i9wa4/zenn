---
title: "databricks-goose-starter „Çí‰Ωú„Çä„Åæ„Åó„Åü ‚Äï ÈñãÁô∫ËÄÖ„Åß„Å™„Åè„Å¶„ÇÇ‰Ωø„Åà„Çã AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞Áí∞Â¢É"
emoji: "ü¶Ü"
type: "tech"
topics:
  - "databricks"
  - "devcontainer"
  - "goose"
  - "jupyter"
  - "mcp"
publication_name: "genda_jp"
published: false
---

## 1. „ÅØ„Åò„ÇÅ„Å´

Ê†™Âºè‰ºöÁ§æGENDA „Éá„Éº„Çø„Ç®„É≥„Ç∏„Éã„Ç¢ / MLOps „Ç®„É≥„Ç∏„Éã„Ç¢„ÅÆ uma-chan „Åß„Åô„ÄÇ

ÊäÄË°ìÈÉ®ÈñÄ‰ª•Â§ñ„ÅÆ„É°„É≥„Éê„Éº („Éá„Éº„Çø„Ç¢„Éä„É™„Çπ„Éà„Å™„Å©) „Å´„ÇÇ AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞Áí∞Â¢É„ÇíÊèê‰æõ„Åó„Åü„ÅÑ„ÄÅ„Åù„Çì„Å™Ë™≤È°å„ÇíËß£Ê±∫„Åô„Çã Dev Container „ÉÜ„É≥„Éó„É¨„Éº„Éà„Çí OSS „Å®„Åó„Å¶ÂÖ¨Èñã„Åó„Åæ„Åó„Åü„ÄÇ

@[card](https://github.com/i9wa4/databricks-goose-starter)

Êú¨Ë®ò‰∫ã„Åß„ÅØ„ÄÅ„Åì„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅÆÊ©üËÉΩ„Å®‰Ωø„ÅÑÊñπ„ÇíÁ¥π‰ªã„Åó„Åæ„Åô„ÄÇ

Êú¨Ë®ò‰∫ã„ÅØ‰∏ãË®ò„Ç§„Éô„É≥„Éà„Åß„ÅÆÁôªÂ£áÂÜÖÂÆπ„ÇíË£úÂÆå„Åô„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ

@[card](https://jedai.connpass.com/event/379174/)

## 2. Ëß£Ê±∫„Åó„Åü„ÅÑË™≤È°å

### 2.1. AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆÊôÆÂèä„ÅÆÂ£Å

AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÁµÑÁπîÂÜÖ„Å´Â±ïÈñã„Åó„Çà„ÅÜ„Å®„Åô„Çã„Å®„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Ë™≤È°å„Åå„ÅÇ„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ

- Áí∞Â¢ÉÊßãÁØâ„ÅÆ„Éè„Éº„Éâ„É´„ÅåÈ´ò„ÅÑ
- Ë™çË®ºË®≠ÂÆö„ÅåË§áÈõë (API „Ç≠„ÉºÁÆ°ÁêÜ„ÄÅService Principal ‰ΩúÊàê„Å™„Å©)
- ÊäÄË°ìÈÉ®ÈñÄ‰ª•Â§ñ„ÅÆ„É°„É≥„Éê„Éº„Å´„ÅØÊï∑Â±Ö„ÅåÈ´ò„ÅÑ

### 2.2. „Åì„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅÆËß£Ê±∫Á≠ñ

databricks-goose-starter „ÅØ„Åì„Çå„Çâ„ÅÆË™≤È°å„ÇíËß£Ê±∫„Åó„Åæ„Åô„ÄÇ

- **Dev Container „ÅßÁí∞Â¢ÉÊßãÁØâ„ÇíËá™ÂãïÂåñ**: „ÄåReopen in Container„Äç„Çí„ÇØ„É™„ÉÉ„ÇØ„Åô„Çã„Å†„Åë
- **OAuth U2M Ë™çË®º„ÅßË§áÈõë„Å™Ë™çË®ºË®≠ÂÆö‰∏çË¶Å**: „Éñ„É©„Ç¶„Ç∂„ÅßË™çË®º„Åô„Çã„Å†„Åë„ÅßÂÆå‰∫Ü
- **Goose ÁµåÁî±„ÅßÂØæË©±ÁöÑ„Å´Êìç‰Ωú**: Ëá™ÁÑ∂Ë®ÄË™û„Åß Notebook ÂÆüË°å„ÇÑ SQL ÂÆüË°å„Çí‰æùÈ†º
- **„É≠„Éº„Ç´„É´„Å´ Python „ÇÑ Spark „Çí„Ç§„É≥„Çπ„Éà„Éº„É´‰∏çË¶Å**: „Åô„Åπ„Å¶ Databricks ‰∏ä„ÅßÂÆüË°å

ÈñãÁô∫ËÄÖ„Åß„Å™„Åè„Å¶„ÇÇ AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Ç®„Éº„Ç∏„Çß„É≥„Éà„Åå‰Ωø„Åà„ÇãÁí∞Â¢É„ÇíÁõÆÊåá„Åó„Åæ„Åó„Åü„ÄÇ

## 3. ÂØæË±°„É¶„Éº„Ç∂„Éº

„Åì„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅØ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™„É¶„Éº„Ç∂„Éº„Å´ÈÅ©„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

- „Éá„Éº„Çø„Ç¢„Éä„É™„Çπ„Éà„ÄÅ„Éì„Ç∏„Éç„Çπ„É¶„Éº„Ç∂„Éº
- „Ç§„É≥„Çø„Éº„É≥„ÄÅÂ§ñÈÉ®ÂçîÂäõËÄÖ
- ‰ªä„Åô„Åê AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞Áí∞Â¢É„Çí‰Ωø„ÅÑ„Åü„ÅÑ‰∫∫

ÈÄÜ„Å´ÈÅ©„Åó„Å¶„ÅÑ„Å™„ÅÑ„ÅÆ„ÅØ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™„Ç±„Éº„Çπ„Åß„Åô„ÄÇ

- Â§ßÈáè„Å´„Éà„Éº„ÇØ„É≥„ÇíÊ∂àË≤ª„Åô„Çã„Éò„Éì„Éº„É¶„Éº„Ç∂„Éº
- „É™„Ç¢„É´„Çø„Ç§„É†„Åß„ÅÆÂà©Áî®Âà∂Èôê„ÅåÂøÖË¶Å„Å™Â†¥Âêà

## 4. ‰Ωú„Å£„Åü„ÇÇ„ÅÆ

Databricks Mosaic AI Gateway „Çí LLM „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Å®„Åó„Å¶‰Ωø„ÅÜ Goose (AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Ç®„Éº„Ç∏„Çß„É≥„Éà) „ÅÆ Dev Container „ÉÜ„É≥„Éó„É¨„Éº„Éà„Åß„Åô„ÄÇ

### 4.1. Goose + Mosaic AI Gateway

Goose „ÅØ Block Á§æ„ÅåÈñãÁô∫„Åó„Åü AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Ç®„Éº„Ç∏„Çß„É≥„Éà„Åß„Åô„ÄÇDatabricks Mosaic AI Gateway ÁµåÁî±„Åß Claude „ÇÑ GPT „Å™„Å©„ÅÆ LLM „ÇíÂà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ

@[card](https://github.com/block/goose)

```console
goose
```

Goose „ÅØ‰ª•‰∏ã„ÅÆ„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ

- „Ç≥„Éû„É≥„ÉâÂÆüË°å (`uv run jupyter execute` „ÇíÂê´„ÇÄ)
- Notebook „ÅÆË™≠„ÅøÊõ∏„Åç
- „Ç≥„Éº„Éâ„Éô„Éº„Çπ„Å®„ÅÆÂØæË©±

ÊäÄË°ìÈÉ®ÈñÄ‰ª•Â§ñ„ÅÆ„É°„É≥„Éê„Éº„Åß„ÇÇ„ÄÅËá™ÁÑ∂Ë®ÄË™û„Åß„Äå„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„ÅÆÈõÜË®à„Çí„Åó„Å¶„Äç„ÄåNotebook „ÇíÂÆüË°å„Åó„Å¶„Äç„Å®‰æùÈ†º„Åô„Çã„Å†„Åë„ÅßÊìç‰Ωú„Åß„Åç„Åæ„Åô„ÄÇ

### 4.2. MCP Server (ÂØæË©±ÁöÑ„Å™ SQL ÂÆüË°å)

mcp-databricks-server „Åå‰∫ãÂâçË®≠ÂÆö„Åï„Çå„Å¶„Åä„Çä„ÄÅGoose „Åã„Çâ Databricks SQL Warehouse „Å´ÂØæË©±ÁöÑ„Å´„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Åæ„Åô„ÄÇ

- SQL „ÇØ„Ç®„É™ÂÆüË°å (Databricks SQL Warehouse ÁµåÁî±)
- „Ç´„Çø„É≠„Ç∞„ÄÅ„Çπ„Ç≠„Éº„Éû„ÄÅ„ÉÜ„Éº„Éñ„É´‰∏ÄË¶ß (Unity Catalog)
- „ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„Éº„Éû„ÅÆÁ¢∫Ë™ç
- „ÉÜ„Éº„Éñ„É´„É™„Éç„Éº„Ç∏ÊÉÖÂ†±„ÅÆÂèñÂæó

ÂÆâÂÖ®„ÅÆ„Åü„ÇÅ„ÄÅDROP/DELETE/TRUNCATE „Å™„Å©„ÅÆÁ†¥Â£äÁöÑ„Å™ SQL „ÅØ„Éñ„É≠„ÉÉ„ÇØ„Åï„Çå„Åæ„Åô„ÄÇ

@[card](https://github.com/i9wa4/mcp-databricks-server)

SQL „ÇíÊõ∏„Åë„Å™„Åè„Å¶„ÇÇ„ÄåÂ£≤‰∏ä„ÅÆÊúàÂà•Êé®Áßª„ÇíË¶ã„Åü„ÅÑ„Äç„Å®‰ºù„Åà„Çã„Å†„Åë„Åß„ÄÅGoose „ÅåÈÅ©Âàá„Å™„ÇØ„Ç®„É™„ÇíÁîüÊàê„Åó„Å¶ÂÆüË°å„Åó„Åæ„Åô„ÄÇË™çË®º„Åï„ÅàÈÄö„Çå„Å∞ Databricks ‰∏ä„ÅÆ„Éá„Éº„Çø„ÇíÂØæË©±ÁöÑ„Å´Êé¢Á¥¢„Åß„Åç„Åæ„Åô„ÄÇ

### 4.3. jupyter-databricks-kernel (Notebook „ÅÆÂÆåÂÖ®„É™„É¢„Éº„ÉàÂÆüË°å)

jupyter-databricks-kernel „Å´„Çà„Çä„ÄÅNotebook „ÅÆ„Ç≥„Éº„Éâ„Çí Databricks „ÇØ„É©„Çπ„Çø‰∏ä„ÅßÂÆåÂÖ®„É™„É¢„Éº„ÉàÂÆüË°å„Åß„Åç„Åæ„Åô„ÄÇ

```bash
uv run jupyter execute notebooks/sample.ipynb --inplace --kernel_name=databricks
```

@[card](https://github.com/i9wa4/jupyter-databricks-kernel)

„É≠„Éº„Ç´„É´Áí∞Â¢É„ÅÆ„É™„ÇΩ„Éº„Çπ„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„Å™„Åè Goose „Å´„Äå„Åì„ÅÆ Notebook „ÇíÂÆüË°å„Åó„Å¶„Äç„Å®‰æùÈ†º„Åô„Çå„Å∞„ÄÅ‰∏äË®ò„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°å„Åó„Å¶ÁµêÊûú„ÇíÂèñÂæó„Åó„Å¶„Åè„Çå„Åæ„Åô„ÄÇ

## 5. „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÊâãÈ†Ü

### 5.1. ÂâçÊèêÊù°‰ª∂

- VS Code + [Dev Containers Êã°ÂºµÊ©üËÉΩ](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- Docker Desktop „Åæ„Åü„ÅØ Docker „Éá„Éº„É¢„É≥
- Databricks „ÉØ„Éº„ÇØ„Çπ„Éö„Éº„Çπ (Mosaic AI Gateway ÊúâÂäπ)

### 5.2. ‰∫ãÂâçÊ∫ñÂÇô

`.env` „Éï„Ç°„Ç§„É´„ÅØ Dev Container Ëµ∑ÂãïÂâç„Å´‰ΩúÊàê„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

1. „É™„Éù„Ç∏„Éà„É™„Çí„ÇØ„É≠„Éº„É≥
1. `.env.example` „Çí `.env` „Å´„Ç≥„Éî„Éº„Åó„Å¶Á∑®ÈõÜ

```bash
cp .env.example .env
```

Áí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆö

| Â§âÊï∞                          | Ë™¨Êòé                          |
| ---                           | ---                           |
| `DATABRICKS_HOST`             | Databricks „ÉØ„Éº„ÇØ„Çπ„Éö„Éº„Çπ URL |
| `DATABRICKS_CLUSTER_ID`       | Notebook ÂÆüË°åÁî®„ÇØ„É©„Çπ„Çø ID    |
| `DATABRICKS_SQL_WAREHOUSE_ID` | SQL ÂÆüË°åÁî® Warehouse ID       |
| `GOOSE_PROVIDER`              | LLM „Éó„É≠„Éê„Ç§„ÉÄ (databricks)   |
| `GOOSE_MODEL`                 | ‰ΩøÁî®„Åô„Çã„É¢„Éá„É´                |

### 5.3. Dev Container Ëµ∑Âãï

1. VS Code „Åß„É™„Éù„Ç∏„Éà„É™„ÇíÈñã„Åè
1. „ÄåReopen in Container„Äç„Çí„ÇØ„É™„ÉÉ„ÇØ
1. Ëá™Âãï„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó„ÇíÂæÖ„Å§
   - mise „Å®„ÉÑ„Éº„É´ (goose, databricks cli, uv) „ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´
   - Python ‰æùÂ≠òÈñ¢‰øÇ„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´
   - Databricks OAuth Ë™çË®º („Éñ„É©„Ç¶„Ç∂„ÅåÈñã„Åç„Åæ„Åô)
   - Goose „ÅÆ MCP Ë®≠ÂÆö

### 5.4. Goose Ëµ∑Âãï

```bash
goose
```

„Åì„Çå„Å†„Åë„Åß AI „Ç≥„Éº„Éá„Ç£„É≥„Ç∞Áí∞Â¢É„Åå‰Ωø„Åà„Åæ„Åô„ÄÇ

## 6. ‰Ωø„ÅÑÊñπ

Goose „ÇíËµ∑Âãï„Åô„Çã„Å®„ÄÅAI „Ç®„Éº„Ç∏„Çß„É≥„Éà„Å®ÂØæË©±„Åß„Åç„Åæ„Åô„ÄÇ

‰æã

- „Äåsamples.nyctaxi.trips „ÉÜ„Éº„Éñ„É´„ÅÆÊßãÈÄ†„ÇíÊïô„Åà„Å¶„Äç
- „Äånotebooks/sample.ipynb „ÇíÂÆüË°å„Åó„Å¶„Äç
- „Äåpickup_zip „Åî„Å®„ÅÆÂπ≥ÂùáÈÅãË≥É„Çí SQL „ÅßÈõÜË®à„Åó„Å¶„Äç

ÊäÄË°ìÁöÑ„Å™Áü•Ë≠ò„Åå„Å™„Åè„Å¶„ÇÇ„ÄÅ„ÇÑ„Çä„Åü„ÅÑ„Åì„Å®„ÇíËá™ÁÑ∂Ë®ÄË™û„Åß‰ºù„Åà„Çã„Å†„Åë„ÅßÊìç‰Ωú„Åß„Åç„Åæ„Åô„ÄÇ

## 7. OAuth U2M Ë™çË®º„ÅÆ„É°„É™„ÉÉ„Éà

„Åì„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅØ OAuth U2M (User-to-Machine) Ë™çË®º„ÅÆ„Åø„ÅßÂãï‰Ωú„Åó„Åæ„Åô„ÄÇ

ÂæìÊù•„ÅÆÊñπÊ≥ï„Å®„ÅÆÊØîËºÉ

| Ë™çË®ºÊñπÂºè              | Ë®≠ÂÆö„ÅÆË§áÈõë„Åï | „Éà„Éº„ÇØ„É≥ÁÆ°ÁêÜ   | Ê®©ÈôêÁÆ°ÁêÜ     |
| --------------------- | ------------ | -------------- | ------------ |
| Personal Access Token | ‰∏≠           | ÊâãÂãïÊõ¥Êñ∞„ÅåÂøÖË¶Å | „Éà„Éº„ÇØ„É≥Âçò‰Ωç |
| Service Principal     | È´ò           | Ëá™Âãï           | SP Âçò‰Ωç      |
| OAuth U2M             | ‰Ωé           | Ëá™ÂãïÊõ¥Êñ∞       | „É¶„Éº„Ç∂„ÉºÂçò‰Ωç |

OAuth U2M „ÅÆ„É°„É™„ÉÉ„Éà

- Service Principal „ÅÆ‰ΩúÊàê„Åå‰∏çË¶Å
- ÂÄã‰∫∫„ÅÆË™çË®ºÊÉÖÂ†±„ÅßÂç≥Â∫ß„Å´Âà©Áî®ÈñãÂßã
- ÈñãÁô∫ËÄÖ„Åî„Å®„ÅÆÊ®©Èôê„ÅßÂãï‰Ωú (ÊúÄÂ∞èÊ®©Èôê„ÅÆÂéüÂâá)
- „Éà„Éº„ÇØ„É≥ÁÆ°ÁêÜ„ÅÆÊâãÈñì„Åå‰∏çË¶Å (OAuth Ëá™ÂãïÊõ¥Êñ∞)

Dev Container Ëµ∑ÂãïÊôÇ„Å´ `databricks auth login` „ÅåÂÆüË°å„Åï„Çå„ÄÅ„Éñ„É©„Ç¶„Ç∂„ÅßË™çË®º„Åô„Çã„Å†„Åë„ÅßÂÆå‰∫Ü„Åó„Åæ„Åô„ÄÇË§áÈõë„Å™Ë®≠ÂÆö„ÅØ‰∏ÄÂàá‰∏çË¶Å„Åß„Åô„ÄÇ

## 8. ‰∫àÁÆóÁÆ°ÁêÜ (Budget Monitoring)

„É¶„Éº„Ç∂„ÉºÂçò‰Ωç„ÅÆ„Éà„Éº„ÇØ„É≥Ê∂àË≤ªÈáè„ÇíÁõ£Ë¶ñ„Åó„ÄÅ‰∫àÁÆóË∂ÖÈÅéÊôÇ„Å´Ëá™Âãï„Åß„Ç¢„ÇØ„Çª„Çπ„ÇíÂà∂Èôê„Åô„Çã‰ªïÁµÑ„Åø„Çí Databricks Job „ÅßÂÆüË£Ö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

### 8.1. ‰ªïÁµÑ„Åø

- `system.serving.endpoint_usage` „ÉÜ„Éº„Éñ„É´„Åß„Éà„Éº„ÇØ„É≥Ê∂àË≤ªÈáè„ÇíÈõÜË®à
- Mosaic AI Gateway „ÅÆ `rate_limits` API „Åß `calls=0` „ÇíË®≠ÂÆö„Åó„Å¶„É¶„Éº„Ç∂„Éº„Çí„Éñ„É≠„ÉÉ„ÇØ
- ÊúàÂàù„Å´ `rate_limits` „Çí„É™„Çª„ÉÉ„Éà„Åó„Å¶ÂÖ®„É¶„Éº„Ç∂„Éº„ÇíËß£Èô§

### 8.2. budget-monitor Job

‰∫àÁÆóË∂ÖÈÅé„É¶„Éº„Ç∂„Éº„ÇíÊ§úÂá∫„Åó„Å¶„Éñ„É≠„ÉÉ„ÇØ„Åô„Çã Job „Åß„Åô„ÄÇ

```python
"""
Budget Monitor Job - Detects budget overages and blocks users via rate limits.
"""

import os
import sys
from datetime import datetime

import requests

# Configuration
BUDGET_TOKENS = 10_000_000  # 10 million tokens
ENDPOINT_NAME = "databricks-claude-sonnet-4"
WAREHOUSE_ID = "warehouse-id"


def log(message: str):
    """Print timestamped log message."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def get_databricks_credentials():
    """Get Databricks host and token."""
    try:
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.getOrCreate()
        host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")
        from pyspark.dbutils import DBUtils

        dbutils = DBUtils(spark)
        token = (
            dbutils.notebook.entry_point.getDbutils()
            .notebook()
            .getContext()
            .apiToken()
            .get()
        )
        return host, token
    except Exception as e:
        log(f"Warning: Could not get credentials from Spark context: {e}")

    host = os.environ.get("DATABRICKS_HOST")
    token = os.environ.get("DATABRICKS_TOKEN")
    if not host or not token:
        raise ValueError("DATABRICKS_HOST and DATABRICKS_TOKEN must be set")
    return host.rstrip("/"), token


def execute_sql(host: str, token: str, statement: str) -> list:
    """Execute SQL via Statement Execution API."""
    url = f"{host}/api/2.0/sql/statements"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload = {
        "warehouse_id": WAREHOUSE_ID,
        "statement": statement,
        "wait_timeout": "50s",
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    result = response.json()

    if result["status"]["state"] != "SUCCEEDED":
        raise RuntimeError(f"SQL execution failed: {result}")

    return result.get("result", {}).get("data_array", [])


def get_exceeded_users_with_usage(host: str, token: str) -> list:
    """Get users who exceeded budget with their token usage."""
    query = f"""
    SELECT
        requester,
        SUM(input_token_count) as input_tokens,
        SUM(output_token_count) as output_tokens,
        SUM(input_token_count + output_token_count) as total_tokens
    FROM system.serving.endpoint_usage
    WHERE request_time >= date_trunc('month', current_date())
    GROUP BY requester
    HAVING SUM(input_token_count + output_token_count) >= {BUDGET_TOKENS}
    ORDER BY total_tokens DESC
    """
    return execute_sql(host, token, query)


def block_users(host: str, token: str, users: list) -> bool:
    """Set rate_limits calls=0 for exceeded users. Returns success status."""
    if not users:
        log("No users to block")
        return True

    rate_limits = [
        {"calls": 0, "key": "user", "principal": user, "renewal_period": "minute"}
        for user in users
    ]
    config = {"rate_limits": rate_limits, "usage_tracking_config": {"enabled": True}}

    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}/ai-gateway"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    try:
        response = requests.put(url, headers=headers, json=config)
        response.raise_for_status()
        return True
    except Exception as e:
        log(f"ERROR: Failed to block users: {e}")
        return False


def main():
    print("=" * 60)
    log("Budget Monitor Job started")
    print("=" * 60)

    # Configuration
    print("\n[Configuration]")
    print(f"  Budget threshold: {BUDGET_TOKENS:,} tokens")
    print(f"  Target endpoint:  {ENDPOINT_NAME}")
    print(f"  SQL Warehouse:    {WAREHOUSE_ID}")

    host, token = get_databricks_credentials()
    print(f"  Workspace:        {host}")

    # Query exceeded users
    print("\n[Query Results]")
    log("Querying system.serving.endpoint_usage...")

    exceeded_users = get_exceeded_users_with_usage(host, token)

    if not exceeded_users:
        log("No users exceeding budget found")
    else:
        log(f"Found {len(exceeded_users)} user(s) exceeding budget:")
        print()
        print(f"  {'User':<40} {'Input':<15} {'Output':<15} {'Total':<15}")
        print(f"  {'-' * 40} {'-' * 15} {'-' * 15} {'-' * 15}")
        for row in exceeded_users:
            user, input_t, output_t, total_t = (
                row[0],
                int(row[1]),
                int(row[2]),
                int(row[3]),
            )
            print(f"  {user:<40} {input_t:>14,} {output_t:>14,} {total_t:>14,}")

    # Block users
    print("\n[Block Processing]")
    users_to_block = [row[0] for row in exceeded_users]

    if users_to_block:
        log(f"Blocking {len(users_to_block)} user(s)...")
        success = block_users(host, token, users_to_block)
        if success:
            log("SUCCESS: All users blocked via rate_limits (calls=0)")
        else:
            log("FAILED: Could not block users")
            sys.exit(1)
    else:
        log("No blocking action required")

    # Completion
    print()
    print("=" * 60)
    log("Budget Monitor Job completed")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

### 8.3. budget-monthly-reset Job

ÊúàÂàù„Å´ rate_limits „Çí„É™„Çª„ÉÉ„Éà„Åô„Çã Job „Åß„Åô„ÄÇ

```python
"""
Budget Monthly Reset Job - Resets rate limits at the beginning of each month.
"""

import os
import sys
from datetime import datetime

import requests

# Configuration
ENDPOINT_NAME = "databricks-claude-sonnet-4"


def log(message: str):
    """Print timestamped log message."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def get_databricks_credentials():
    """Get Databricks host and token."""
    try:
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.getOrCreate()
        host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")
        from pyspark.dbutils import DBUtils

        dbutils = DBUtils(spark)
        token = (
            dbutils.notebook.entry_point.getDbutils()
            .notebook()
            .getContext()
            .apiToken()
            .get()
        )
        return host, token
    except Exception as e:
        log(f"Warning: Could not get credentials from Spark context: {e}")

    host = os.environ.get("DATABRICKS_HOST")
    token = os.environ.get("DATABRICKS_TOKEN")
    if not host or not token:
        raise ValueError("DATABRICKS_HOST and DATABRICKS_TOKEN must be set")
    return host.rstrip("/"), token


def get_current_rate_limits(host: str, token: str) -> dict:
    """Get current AI Gateway configuration."""
    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}"
    headers = {"Authorization": f"Bearer {token}"}

    response = requests.get(url, headers=headers)
    response.raise_for_status()
    result = response.json()
    return result.get("ai_gateway", {})


def reset_rate_limits(host: str, token: str) -> bool:
    """Reset rate limits to unblock all users. Returns success status."""
    config = {"rate_limits": [], "usage_tracking_config": {"enabled": True}}

    url = f"{host}/api/2.0/serving-endpoints/{ENDPOINT_NAME}/ai-gateway"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    try:
        response = requests.put(url, headers=headers, json=config)
        response.raise_for_status()
        return True
    except Exception as e:
        log(f"ERROR: Failed to reset rate limits: {e}")
        return False


def main():
    print("=" * 60)
    log("Budget Monthly Reset Job started")
    print("=" * 60)

    # Configuration
    print("\n[Configuration]")
    print(f"  Target endpoint: {ENDPOINT_NAME}")

    host, token = get_databricks_credentials()
    print(f"  Workspace:       {host}")

    # Get current state
    print("\n[Current State]")
    log("Fetching current rate_limits...")

    current_config = get_current_rate_limits(host, token)
    current_limits = current_config.get("rate_limits", [])

    if not current_limits:
        log("No rate_limits currently set")
    else:
        log(f"Found {len(current_limits)} rate_limit(s):")
        for limit in current_limits:
            principal = limit.get("principal", "N/A")
            calls = limit.get("calls", "N/A")
            key = limit.get("key", "N/A")
            print(f"  - {principal} (key={key}, calls={calls})")

    # Reset rate limits
    print("\n[Reset Processing]")
    log("Resetting rate_limits to empty...")

    success = reset_rate_limits(host, token)

    if success:
        log("SUCCESS: Rate limits cleared")
        if current_limits:
            log(f"Unblocked {len(current_limits)} user(s)")
        else:
            log("No users were blocked")
    else:
        log("FAILED: Could not reset rate limits")
        sys.exit(1)

    # Completion
    print()
    print("=" * 60)
    log("Budget Monthly Reset Job completed")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

## 9. „Åä„Çè„Çä„Å´

Issue „Åß„Éê„Ç∞Â†±Âëä„ÇÑÊ©üËÉΩÊèêÊ°à„Å™„Å©„ÅäÊ∞óËªΩ„Å´„Å©„ÅÜ„Åû„ÄÇ

@[card](https://github.com/i9wa4/databricks-goose-starter)

### 9.1. Èñ¢ÈÄ£Ë®ò‰∫ã

@[card](https://zenn.dev/genda_jp/articles/2025-12-13-jupyter-databricks-kernel-oss-dev)

@[card](https://zenn.dev/genda_jp/articles/2025-12-19-databricks-notebook-ai-ready)

@[card](https://zenn.dev/genda_jp/articles/2025-12-24-mcp-databricks-server-v2)

### 9.2. Èñ¢ÈÄ£„Éó„É≠„Ç∏„Çß„ÇØ„Éà

@[card](https://github.com/block/goose)

@[card](https://github.com/i9wa4/mcp-databricks-server)

@[card](https://github.com/i9wa4/jupyter-databricks-kernel)
